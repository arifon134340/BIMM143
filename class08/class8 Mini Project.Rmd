---
title: "class8: Mini Project"
author: 'Ari_Fon (PID: A15390446)'
date: "2/10/2022"
output: pdf_document
---

#Unsupervised Learning Analysis of Human Breast Cancer Cells

##Exploratory data analysis

###Read the data into R using `read.csv()`

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```

###Remove diagnosis column

```{r}
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

###Save diagnosis column as a new vector

```{r}
diagnosis <- as.factor(wisc.df[,1])
head(diagnosis)
```

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
sum(grepl("M", diagnosis))
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
sum(grepl("_mean", names(wisc.data)))
```

##Principal Component Analysis

##Performing PCA

###Check column means and standard deviation

```{r}
colMeans(wisc.data)
apply(wisc.data, 2, sd)
```

###Perform PCA

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 principal components (PC3) 

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 principal components (PC7)

###Creating a Biplot `biplot()` of wisc.pr function

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

This plot is difficult to understand since everything is all bunched together and it is hard to read or look at data in the plot. 


### Now I will make result: "PCA Plot" (a.k.a. "score plot PC1 vs PC2 plot)

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis,  xlab = "PC1", ylab = "PC2")
```

### Now I will do the same to compare PC1 and PC3

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,3], col=diagnosis, xlab = "PC1", ylab = "PC3")
```


###Attempt with ggplot function

```{r}
#create dataframe with ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

#load ggplot2 package
library(ggplot2)

#make a scatterplot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

###Variance Explained

###Scree Plot Attempt

```{r}
#calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
```{r}
# Variance explained by each principal component: pve
pve <- pr.var/ sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )

```

###Optional ggplot graph exploration

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

##Comunicating PCA 

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
grep("concave.points_mean", names(wisc.pr$rotation[,1]))
wisc.pr$rotation[8,1]
```

>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
summary(wisc.pr)
```

Minimum is PC5 to explain 80% of variance of data


##Heriarchical Clustering

First we scale the data
```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

Calculate (Euclidean) distances between all pairs of observations

```{r}
data.dist <- dist(data.scaled)
head(data.dist)
```

```{r}
wisc.hclust <- hclust(data.dist, method= "complete")
```

###Result of Heirarchical clustering

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19.5, col="red", lty=2)
```

###Selecting number of Clusters
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

###Using different methods

>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

I chose "ward.D2" because it makes it easier for me to see the different groups and its a little easier to distinguish between possible clusters. 
```{r}
plot(hclust(data.dist, method= "ward.D2"))
```

##Combining Methods

```{r}
pcdist <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(pcdist, method="ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
plot(wisc.pr$x[,1:2], col=grps)
table(diagnosis)
table(diagnosis, grps)
```







